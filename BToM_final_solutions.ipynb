{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Theory Of Mind Problem Set\n",
    "\n",
    "\n",
    "1. [Building The Model](#Distance-Calc)\n",
    "  1. [Initializing The Model](#init)\n",
    "  1. [Distance-Calc](#Distance-Calc)\n",
    "  2. [Getting Points Within The Range](#Within-Range)\n",
    "2. [The Mind Model](#Mind-Model)\n",
    "  1. [Updating Beliefs](#updating-beliefs)\n",
    "  2. [Defining Transition and Action](#transition-matrix)\n",
    "  3. [Best Policy](#best-policy)\n",
    "  4. [Inferring Intent](#inferring-intent)\n",
    "3. [Analysis of Bayesian Theory Of Mind](#analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model\n",
    "\n",
    "<img src=\"scenario.png\"/>\n",
    "\n",
    "\n",
    "Before we start constructing a way to infer intent, we first need to find a way to represent our \"world\", the space in which we will infer intent. In this case, our world is a 15x15 square grid that our agent (in this case, Mark Watney) can traverse around. Scattered in known locations on the grid are resources A B and C. Mark is trying to retrieve these resources, but we're not sure which one he wants. This is where the intent comes in!\n",
    "\n",
    "To be more technical and specific, we will write some class Mind that keeps track of our model's estimation of Mark's intent, belief about the world, and how likely he's going for a certain resource given his location.\n",
    "\n",
    "Below, we'll initialize this Mind class, then dive a bit deeper into how to use it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize The Mind (5 pts) <a id=\"temporal-word-problem\"/>\n",
    "\n",
    "Our first step is to initialize the Bayesian Theory of Mind. Below is an implementation of the class we'll use to model our BToM, more in particular the init.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mind:\n",
    "    def __init__(self):\n",
    "        # where resources are\n",
    "        self.world_loc = [(10,0), (0,9), (6,10)]\n",
    "        self.map_length = 15\n",
    "        self.world_state = 'ABC'\n",
    "\n",
    "        # possible assignments to world\n",
    "        self.beliefs_worlds = ['ABC', 'ACB', 'BAC', 'BCA', 'CAB', 'CBA']\n",
    "        # belief that the orientation is the above\n",
    "        self.beliefs = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]\n",
    "\n",
    "        self.intents = {'A': 1/3,\n",
    "                        'B': 1/3,\n",
    "                        'C': 1/3}\n",
    "\n",
    "        self.state = (0,0)\n",
    "\n",
    "        self.states = []\n",
    "        for i in range(self.map_length):\n",
    "            for j in range(self.map_length):\n",
    "                self.states.append((i,j))\n",
    "\n",
    "        self.gamma = 0.5\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-557370c6a2aeba55",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Let's talk about the initialization below. Most of the initialization is self explanatory. The world and map length gives us what the map looks like, and the \"actual world\" gives us where the resources really are. In this case, it's \"ABC\", which means that A is at (10,0), B is at (0,9), and C is at (6,10).\n",
    "\n",
    "belief_worlds gives us all possible assignments to worlds. state tells us where we are in the world, whereas states tell us all possible states (we use this and gamma in value iteration, it's not quite the focus of this pset so don't worry too much about it)\n",
    "\n",
    "Then, we initialize the self.beliefs and self.intents. Notice that all the probabilites are initialized to the same thing. What is this distribution called? What is the benefit of initializing our beliefs and intents in this way, and can you think of any other ways to do it?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-193d7e69e88e8cdd",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "ANSWER: The distribution is a uniform distribution. Any attempt at improving the model could be considered an answer for the second part, for example, taking existing intents and factoring that into our initial distribution, or if the observer is aware that the agent has some beliefs, that could also be taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Calculation (5 pts) <a id=\"distance-calc\"/>\n",
    "\n",
    "In order for our model to understand anything, we need to be able to calculate distances between objects on our field. Write a function here to calculate the distance between two points p1 and p2 (both tuples of the form x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to get distance\n",
    "def get_distance(p1, p2):\n",
    "    return ((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that our Distance Calculation Works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5be901e3420da294",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "assert_equal(get_distance((1,0), (2,0)), 1)\n",
    "assert_equal(get_distance((1,1), (2,2)), (2)**0.5)\n",
    "assert_equal(get_distance((1,0), (1,2)), 2)\n",
    "\n",
    "print (\"success\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Points Within A Range (10 pts) <a id=\"within-range\"/>\n",
    "\n",
    "Now, our next step is to be able to find the points that Mark can see when he is at some location. Implement the method below, that given a Mind object and location, returns all the resources that Mark can see. (5 squares away in x or y direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def within_range(mind, position):\n",
    "    \"\"\" \n",
    "    Given a mind object and location, outputs the (x,y) coordinates of resources near the location\n",
    "    \n",
    "    Input: mind - a mind object which contains location of all the resources on map\n",
    "           position - current position on map\n",
    "    Output: locs - a list of locations (in x,y) available from current position\n",
    "    \"\"\"\n",
    "    # should return resource positions that are visible from current position\n",
    "#     locs = []\n",
    "    \n",
    "#     return locs\n",
    "\n",
    "    locs = []\n",
    "    for i in range(len(mind.world_loc)):\n",
    "        loc = mind.world_loc[i]\n",
    "        if abs(position[0]-loc[0])<=5 and abs(position[1]-loc[1])<=5:\n",
    "            locs.append(loc)\n",
    "    return locs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-069f8f72f4ccd00b",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "my_mind = Mind()\n",
    "\n",
    "assert_equal(within_range(my_mind, (8,0)), [(10, 0)])\n",
    "assert_equal(within_range(my_mind, (0,0)), [])\n",
    "assert_equal((6,10) in within_range(my_mind, (3,9)) and (0,9) in within_range(my_mind, (3,9)), True)\n",
    "print (\"success\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mind Model\n",
    "\n",
    "Next, we need to actually write some useful functions that will use the mind model to get intent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Beliefs (20 pts) <a id=\"updating-beliefs\"/>\n",
    "\n",
    "In this section, we'll work on updating what our BToM thinks that Mark believes.\n",
    "\n",
    "The function below is passed a mind instance and the current state. We should update our probability distribution of what we think Mark believes given this new information.\n",
    "\n",
    "Here are the steps you should follow:\n",
    "- Obtain all locations of resources that are within range of our state (use a function you made!)\n",
    "For each of the locations you will then:\n",
    "- Get the resource that is at that location in the actual world\n",
    "- Go through all the 6 possible worlds, keep track of which are consistent and inconsistent with the resource being in that location.\n",
    "- For each of the consistent worlds, multiply the previous belief in that world by 0.9 over the number of consistent worlds.\n",
    "- Similarly for the inconsistent worlds, multiply the previous belief in that world by 0.1 over the number of inconsistent worlds.\n",
    "- Normalize the new beliefs\n",
    "\n",
    "Hint: If the position of the resource we just discovered matches some possible state of the world, Mark is also much more likely to think that that is the true state of the world.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beliefs_update(mind, state):\n",
    "    \"Update beliefs\"\n",
    "    near_locations = within_range(mind, state)\n",
    "\n",
    "    # figure out which locations you can see\n",
    "    # update beliefs for the ones close to you\n",
    "\n",
    "    #redistribute beliefs\n",
    "\n",
    "    for loc in near_locations:\n",
    "        i = mind.world_loc.index(loc)\n",
    "        resource = mind.world_state[i]\n",
    "#         print (\"observed resource %s\" % resource)\n",
    "        consistently_observed_worlds = []\n",
    "        inconsistently_observed_worlds = []\n",
    "        for j in range(len(mind.beliefs_worlds)):\n",
    "            world = mind.beliefs_worlds[j]\n",
    "            if world[i] == resource:\n",
    "                consistently_observed_worlds.append(j)\n",
    "            else:\n",
    "                inconsistently_observed_worlds.append(j)\n",
    "        factor = len(consistently_observed_worlds)\n",
    "        sum_of_consistently_observed = sum([mind.beliefs[i] for i in consistently_observed_worlds])\n",
    "        sum_of_inconsistently_observed = sum([mind.beliefs[i] for i in inconsistently_observed_worlds])\n",
    "        for i in range(len(mind.beliefs)):\n",
    "            if i in consistently_observed_worlds:\n",
    "                mind.beliefs[i] = 0.9 * mind.beliefs[i]/sum_of_consistently_observed\n",
    "            else:\n",
    "                mind.beliefs[i] = 0.1 * mind.beliefs[i]/sum_of_inconsistently_observed\n",
    "\n",
    "        mind.beliefs = [float(i)/sum(mind.beliefs) for i in mind.beliefs]\n",
    "#         print(\"new beliefs %s \" % mind.beliefs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e638d4dae4812c55",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_equal, assert_true\n",
    "\n",
    "my_mind = Mind()\n",
    "\n",
    "beliefs_update(my_mind, (10,0))\n",
    "assert_equal(my_mind.beliefs[0], 0.45)\n",
    "beliefs_update(my_mind, (0,9))\n",
    "assert_true(abs(my_mind.beliefs[0]-0.85)<0.01)\n",
    "\n",
    "print (\"success\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Transition and Actions (20 pts) <a id=\"transition-matrix\"/>\n",
    "\n",
    "In addition to wanting to figure out Mark's intent, our model also models Mark's mind in the context of a planning problem as well. We'd like to know what action Mark wants to take next. Although this also includes the value iteration algorithm as well as getting the best policy (the best actions he should take), we'll be defining just some of the functions necessary to get the best policy.\n",
    "\n",
    "We'll be implementing a function, transition, that giving a state and an action, returns a list of (result state, probability) pairs. \n",
    "\n",
    "Hint: We included two helper function, get_next_state, that gives the next state that would be reached by taking some action, and actions, which gets the next set of possible functions. Use them to help implement transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(mind, state, action):\n",
    "    \"\"\"\n",
    "    Helper function. Given state and action, gives next state that would\n",
    "    be reached. Returns None if off map.\n",
    "    \"\"\"\n",
    "    next_state = None\n",
    "    x = state[0]\n",
    "    y = state[1]\n",
    "\n",
    "    if action == 'right':\n",
    "        if 0<=(x+1)<mind.map_length and 0<=y<mind.map_length:\n",
    "            next_state = (x+1,y)\n",
    "    elif action == 'left':\n",
    "        if 0<=(x-1)<mind.map_length and 0<=y<mind.map_length:\n",
    "            next_state = (x-1,y)\n",
    "    elif action == 'up':\n",
    "        if 0<=x<mind.map_length and 0<=(y-1)<mind.map_length:\n",
    "            next_state = (x,y-1)\n",
    "    elif action == 'down':\n",
    "        if 0<=x<mind.map_length and 0<=(y+1)<mind.map_length:\n",
    "            next_state = (x,y+1)\n",
    "\n",
    "    return next_state\n",
    "\n",
    "\n",
    "def actions(mind, state):\n",
    "    \"Set of actions that can be performed in this state.\"\n",
    "    actions = []\n",
    "    x = state[0]\n",
    "    y = state[1]\n",
    "\n",
    "    if 0<=(x+1)<mind.map_length and 0<=y<mind.map_length:\n",
    "        actions.append('right')\n",
    "    if 0<=(x-1)<mind.map_length and 0<=y<mind.map_length:\n",
    "        actions.append('left')\n",
    "    if 0<=x<mind.map_length and 0<=(y-1)<mind.map_length:\n",
    "        actions.append('up')\n",
    "    if 0<=x<mind.map_length and 0<=(y+1)<mind.map_length:\n",
    "        actions.append('down')\n",
    "\n",
    "    return actions\n",
    "\n",
    "    \n",
    "def transition(mind, state, action):\n",
    "    \"\"\"\n",
    "    Transition model.  From a state and an action, return a list\n",
    "    of (result-state, probability) pairs.\n",
    "    \"\"\"\n",
    "    pos_actions = actions(mind, state)\n",
    "    pairs = []\n",
    "\n",
    "    action_valid = False\n",
    "    next_state = get_next_state(mind, state, action)\n",
    "\n",
    "    x = state[0]\n",
    "    y = state[1]\n",
    "\n",
    "    if next_state:\n",
    "        pairs.append((next_state, 0.9))\n",
    "        remaining = len(pos_actions) - 1\n",
    "        prob = 0.1 / remaining\n",
    "        for a in pos_actions:\n",
    "            if a != action:\n",
    "                pairs.append((get_next_state(mind, state, a), prob))\n",
    "    else:\n",
    "        prob = 1.0 / len(pos_actions)\n",
    "        for a in pos_actions:\n",
    "            pairs.append((get_next_state(mind, state, a), prob))\n",
    "\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((1, 2), 0.9), ((2, 1), 0.03333333333333333), ((0, 1), 0.03333333333333333), ((1, 0), 0.03333333333333333)]\n",
      "[((5, 3), 0.9), ((3, 3), 0.03333333333333333), ((4, 2), 0.03333333333333333), ((4, 4), 0.03333333333333333)]\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "my_mind = Mind()\n",
    "print(transition(my_mind, (1,1), 'down'))\n",
    "assert_equal(transition(my_mind, (1,1), 'down'), [((1, 2), 0.9), ((2, 1), 0.03333333333333333), ((0, 1), 0.03333333333333333), ((1, 0), 0.03333333333333333)]\n",
    ")\n",
    "\n",
    "print(transition(my_mind, (4,3), 'right'))\n",
    "assert_equal(transition(my_mind, (4,3), 'right'), [((5, 3), 0.9), ((3, 3), 0.03333333333333333), ((4, 2), 0.03333333333333333), ((4, 4), 0.03333333333333333)]\n",
    ")\n",
    "\n",
    "\n",
    "print (\"success\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Policy <a id=\"best-policy\"/>\n",
    "\n",
    "Again, Mark's problem, to him, can be seen as a planning problem, since each step, he wants to choose the best action that will take him to the goal. We can calculate the best policy below, using reward and value iteration as mentioned in lecture. Just run our cases and examine the output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(mind, state, world):\n",
    "    \"Return a numeric reward for this state.\"\n",
    "    if (state in mind.world_loc):\n",
    "        i = mind.world_loc.index(state)\n",
    "        resource = world[i]\n",
    "        return 10*mind.intents[resource]\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def value_iteration(mind, epsilon=0.001):\n",
    "    \"Solving by value iteration.\"\n",
    "\n",
    "    U1 = dict([(s, 0) for s in mind.states])\n",
    "    R, T, gamma = reward, transition, mind.gamma\n",
    "\n",
    "    for i in range(10):\n",
    "        U = U1.copy()\n",
    "        delta = 0\n",
    "        for w_i in range(len(mind.beliefs_worlds)):\n",
    "            for s in mind.states:\n",
    "                U1[s] = R(mind, s, mind.beliefs_worlds[w_i]) + gamma * max([sum([mind.beliefs[w_i] * p * U[s1] for (s1, p) in T(mind, s, a)])\n",
    "                                            for a in actions(mind, s)])\n",
    "                delta = max(delta, abs(U1[s] - U[s]))\n",
    "        if delta < epsilon * (1 - gamma) / gamma:\n",
    "             return U\n",
    "    return U\n",
    "\n",
    "def argmax(keys, f):\n",
    "    \"Helper function to get argmax.\"\n",
    "    return max(keys, key=f)\n",
    "\n",
    "def expected_utility(mind, action, state, U):\n",
    "    \"The expected utility of doing a in state s, according to the MDP and U.\"\n",
    "    return sum([p * U[s1] for (s1, p) in transition(mind, state, action)])\n",
    "\n",
    "def best_policy(mind, U):\n",
    "    \"\"\"\n",
    "    Given an MDP and a utility function U, determine the best policy,\n",
    "    as a mapping from state to action.\n",
    "    \"\"\"\n",
    "    pi = {}\n",
    "    for s in mind.states:\n",
    "        pi[s] = argmax(actions(mind,s), lambda a:expected_utility(mind, a, s, U))\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n",
      "right\n",
      "up\n"
     ]
    }
   ],
   "source": [
    "my_mind = Mind()\n",
    "\n",
    "beliefs_update(my_mind, (1,0))\n",
    "U = value_iteration(my_mind)\n",
    "policy = best_policy(my_mind, U)\n",
    "print (policy[(0,1)])\n",
    "print (policy[(2,2)])\n",
    "print (policy[(9,3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring Intent (20 pts) <a id=\"inferring-intent\"/>\n",
    "\n",
    "Last thing! We need to put it together and infer the intent of Mark using the functions we've defined!\n",
    "\n",
    "In this function, we're passed in a Mind object. Using it, we should get the total probability (normalized) that Mark's intent is to get a particular resource.\n",
    "\n",
    "Steps:\n",
    "- Based on the action Mark takes next, get the next state\n",
    "- For each of the resource positions in the actual world orientation, get the difference in distance Mark is from each of the resource positions based on the state before and after his action. Note: some of these may be negative.\n",
    "- For each of the resources, taking into account the belief distribution in all the different worlds, we want to calculate a score, based on the distance it is from Mark in that world and the belief in that world (to simplify, you can simply multiply the distance by the belief).\n",
    "- Update the old intents by adding this score, discounted by a factor of 0.3. Ensure that the values do not exceed 1 or go below 0.\n",
    "- Finally, normalize the new intents.\n",
    "\n",
    "More specifically, you should mutate mind.intents and update it with the new intents! We've already implemented the step for receiving an observation, and updating other propertiess, but now you must finish it with update_intents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intents_update(mind, action):\n",
    "    \"Update intents.\"\n",
    "    \"\"\"\n",
    "        belief: [Pr(ABC), Pr(ACB), Pr(BAC), Pr(BCA), Pr(CAB), Pr(CBA)]\n",
    "        state: current location on the grid\n",
    "        action: action he takes\n",
    "    \"\"\"\n",
    "\n",
    "    next_state = get_next_state(mind, mind.state, action)\n",
    "\n",
    "    # difference in distance that this action takes us, or how much closer or farther\n",
    "    # we get from the 3 resource locations\n",
    "    dists = []\n",
    "\n",
    "    for resource_pos in mind.world_loc:\n",
    "        dist1 = get_distance(resource_pos, mind.state)\n",
    "        dist2 = get_distance(resource_pos, next_state)\n",
    "        diff_dist = dist1-dist2\n",
    "        dists.append(diff_dist)\n",
    "\n",
    "    new_intents_scores = {'A': 0, 'B': 0, 'C': 0}\n",
    "\n",
    "    for i in range(len(mind.beliefs_worlds)):\n",
    "        world = mind.beliefs_worlds[i]\n",
    "        belief = mind.beliefs[i]\n",
    "        for i in range(3):\n",
    "            r = world[i]\n",
    "            new_intents_scores[r] += dists[i] * belief\n",
    "\n",
    "    new_intents = {}\n",
    "    factor = 0.3\n",
    "    sum_new_intents = 0\n",
    "    for r in mind.intents:\n",
    "        new_intents[r] = max(mind.intents[r] + factor * new_intents_scores[r], 0)\n",
    "        sum_new_intents += new_intents[r]\n",
    "\n",
    "    mind.intents = {'A': new_intents['A']/sum_new_intents, 'B': new_intents['B']/sum_new_intents, 'C': new_intents['C']/sum_new_intents}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def receive_observation(mind, action):\n",
    "    \"Receive observation, update model, and get updated intents.\"\n",
    "    mind.state = get_next_state(mind, mind.state, action)\n",
    "    beliefs_update(mind, mind.state)\n",
    "#     U = value_iteration(mind)\n",
    "#     policy = best_policy(mind, U)\n",
    "    intents_update(mind, action)\n",
    "    print ('intents: ', mind.intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark goes to A, but turns back around.\n",
      "intents:  {'A': 0.3333333333333333, 'B': 0.3333333333333333, 'C': 0.3333333333333333}\n",
      "intents:  {'A': 0.3333333333333333, 'B': 0.3333333333333333, 'C': 0.3333333333333333}\n",
      "intents:  {'A': 0.3333333333333333, 'B': 0.3333333333333333, 'C': 0.3333333333333333}\n",
      "intents:  {'A': 0.3333333333333333, 'B': 0.3333333333333333, 'C': 0.3333333333333333}\n",
      "intents:  {'A': 0.5146189448302096, 'B': 0.24269052758489512, 'C': 0.24269052758489512}\n",
      "intents:  {'A': 0.33483652227777927, 'B': 0.33258173886111037, 'C': 0.33258173886111037}\n",
      "intents:  {'A': 0.09380125471630178, 'B': 0.45309937264184913, 'C': 0.45309937264184913}\n",
      "intents:  {'A': 0.0, 'B': 0.5, 'C': 0.5}\n",
      "intents:  {'A': 0.0, 'B': 0.5, 'C': 0.49999999999999994}\n",
      "---------------------------\n",
      "Mark goes to A, but then starts going to C.\n",
      "intents:  {'A': 0.3333333333333333, 'B': 0.3333333333333333, 'C': 0.3333333333333333}\n",
      "intents:  {'A': 0.3333333333333333, 'B': 0.3333333333333333, 'C': 0.3333333333333333}\n",
      "intents:  {'A': 0.3333333333333333, 'B': 0.3333333333333333, 'C': 0.3333333333333333}\n",
      "intents:  {'A': 0.3333333333333333, 'B': 0.3333333333333333, 'C': 0.3333333333333333}\n",
      "intents:  {'A': 0.5146189448302096, 'B': 0.24269052758489512, 'C': 0.24269052758489512}\n",
      "intents:  {'A': 0.3179626454337534, 'B': 0.34101867728312335, 'C': 0.34101867728312335}\n",
      "intents:  {'A': 0.16010715450152205, 'B': 0.41994642274923905, 'C': 0.41994642274923905}\n",
      "intents:  {'A': 0.023416797388372417, 'B': 0.4882916013058137, 'C': 0.4882916013058137}\n",
      "intents:  {'A': 0.0, 'B': 0.48073323324749934, 'C': 0.5192667667525007}\n",
      "intents:  {'A': 0.0, 'B': 0.4558231780201364, 'C': 0.5441768219798637}\n",
      "intents:  {'A': 0.0, 'B': 0.42456415496188665, 'C': 0.5754358450381134}\n",
      "intents:  {'A': 0.0, 'B': 0.38523647163635155, 'C': 0.6147635283636486}\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "# goes to A but turns back around.\n",
    "my_mind = Mind()\n",
    "print(\"Mark goes to A, but turns back around.\")\n",
    "receive_observation(my_mind, 'right')\n",
    "receive_observation(my_mind, 'right')\n",
    "receive_observation(my_mind, 'right')\n",
    "receive_observation(my_mind, 'right')\n",
    "receive_observation(my_mind, 'right')\n",
    "receive_observation(my_mind, 'left')\n",
    "receive_observation(my_mind, 'left')\n",
    "receive_observation(my_mind, 'left')\n",
    "receive_observation(my_mind, 'left')\n",
    "\n",
    "print (\"---------------------------\")\n",
    "\n",
    "# goes to A, but then starts going to C.\n",
    "my_mind = Mind()\n",
    "print(\"Mark goes to A, but then starts going to C.\")\n",
    "receive_observation(my_mind, 'right')\n",
    "receive_observation(my_mind, 'right')\n",
    "receive_observation(my_mind, 'right')\n",
    "receive_observation(my_mind, 'right')\n",
    "receive_observation(my_mind, 'right')\n",
    "receive_observation(my_mind, 'down')\n",
    "receive_observation(my_mind, 'down')\n",
    "receive_observation(my_mind, 'down')\n",
    "receive_observation(my_mind, 'down')\n",
    "receive_observation(my_mind, 'down')\n",
    "receive_observation(my_mind, 'down')\n",
    "receive_observation(my_mind, 'down')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitatively describe what you saw when we ran our simulation. How did the intents change as we progressed through time? Can you explain what happened, and was it consistent with the bayesian theory of mind? Try to use how we used belief in your answer. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f2d08786a4de6ee0",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first simulation, we saw Mark approach a resource, and then leave. What does this say about his intents and beliefs at that point? (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3f76c3c07addaae5",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "ANSWER: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Bayesian Theory of Mind (10 pts) <a id=\"distance-calc\"/>\n",
    "\n",
    "In the box below, highlight some of the advantages and disadvantages of Bayesian Theory of Mind. This is open ended! What makes it different from other approaches, and why does that make it better? Are there any drawbacks you can think of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e982caeeb74c73ac",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
